[{"RelPermalink":"/posts/africa/","content":"Route Planning // TODO Planning\nStory // TODO Story\n","summary":"Around africa on a motorbike","title":"Round trip Africa"},{"RelPermalink":"/posts/debugging-unifi-connections/","content":"Introduction Having SSH access to the server you are battling with is an invaluable tool - no matter how feature-rich the console may be. In the few past days, we (my father and I) faced connectivity issues on a corporate network where a specific client (a POS device) would not stay attached to the nearest access point but instead hop to an AP further away. That caused connectivity problems with slow/failed transactions. It finally proved to be a miss-configuration issue (as it always is) but had we had proper logging, we could have resolved the issue much sooner.\nUniFi OS The majority of hardware we use is from Ubiquity, with many different APs of various generations and a UDM PRO tying them all down. Unfortunately, Ubiquity\u0026rsquo;s documentation is sparse, so the information seen in this post was collected from blogs and community forums.\nThe first step is to enable SSH access to the UDM through the settings and set a password. Then SSH root@192.168.1.1 and enter the password you\u0026rsquo;ve just set. UniFi OS is based on Buildroot with some basic Linux commands (see those with help) and some UniFi-specific commands.\nFor some weird reason, UI decided we can\u0026rsquo;t have docs for the CLI so we have to do some digging ourselves. As mentioned on the community forum we can find the commands defined in the file syswrapper.sh. Edit that file with vi:\nvi `which syswrapper.sh` Then search for case $cmd in in the file. You can search in vi by hitting / and typing the search term. Don\u0026rsquo;t forget you can exit the file with :q!, which quits without saving. There is a thread on Reddit and a repository on GitHub that documents some of these commands. The following is a snippet from that file.\ncase $cmd in set-tmp-ip) exit_if_fake $cmd $* ;; set-adopt) # set-adopt \u0026lt;url\u0026gt; \u0026lt;authkey\u0026gt; mca-ctrl -t connect -s \u0026#34;$1\u0026#34; -k \u0026#34;$2\u0026#34; ;; set-channel) # set-channel \u0026lt;radio\u0026gt; \u0026lt;channel\u0026gt; # FIXME: dual radio for ath in `ls /proc/sys/net/*/%parent | cut -d \u0026#39;/\u0026#39; -f 5`; do iwconfig $ath channel $1 done ;; # ... Extracting logs We are going to extract log files from two locations:\n/var/log/messages - error logs. # Save the errors to a file cat /var/log/messages \u0026gt; messages.log # Then transfer the file to your machine scp messages.log \u0026lt;user\u0026gt;@\u0026lt;device-name\u0026gt;:\u0026lt;path/to/transfer/to\u0026gt; /data/unifi-core/logs - a directory that contains various UniFi OS logs. # Archive the whole dir tar -zcvf unifi-core-logs.tar.gz /data/unifi-core/logs/ # Then transfer the file to your machine scp messages.log \u0026lt;user\u0026gt;@\u0026lt;device-name\u0026gt;:\u0026lt;path/to/transfer/to\u0026gt; From here on, you are going into a rabbit hole. May your soul find peace because reading these logs drove me crazy.\nAttributions UniFi CLI / SSH commands list All UniFi SSH Commands that You Want to Know How To SSH Into Your UniFi Dream Machine UniFi - How to View Log Files UniFi - Getting Support Files and Logs ","summary":"Accessing Unifi OS logs","title":"Debugging Unifi connections"},{"RelPermalink":"/posts/resolved.conf/","content":"Introduction I encountered a weird issue the other day at work where I could not resolve a specific dev server on my laptop. Despite being logged to the company\u0026rsquo;s VPN and being able to resolve all the other servers, this specific one stubbornly refused. Going through the debugging process of a complex network was not an option, so resolved.conf came to the rescue.\nNetwork Name Resolution configuration files The file resolved.conf is responsible for local DNS and LLNMR (Link-Local Multicast Name Resolution) name resolution. The main file can be found under /etc/systemd and its entries override any defaults. The following configuration was needed for the server to resolve.\nDNS - Define the IPv4/IPv6 entry (or entries) to use as a system DNS servers. Domains - Domains we want to process using the predefined DNS servers. They are processed in the order they are listed, until a match is found. DNS=195.47.208.14 Domains=opap.local open.local After saving the file with the new entry reload the services as such:\n~ ➜ sudo systemctl daemon-reload sudo systemctl restart systemd-networkd sudo systemctl restart systemd-resolved And check that the server now resolves:\n~ ➜ ping tzokerdev.opap.gr -c 4 PING pamdev05.opap.local (10.126.2.45) 56(84) bytes of data. 64 bytes from 10.126.2.45 (10.126.2.45): icmp_seq=1 ttl=63 time=32.9 ms 64 bytes from 10.126.2.45 (10.126.2.45): icmp_seq=2 ttl=63 time=30.5 ms 64 bytes from 10.126.2.45 (10.126.2.45): icmp_seq=3 ttl=63 time=29.5 ms 64 bytes from 10.126.2.45 (10.126.2.45): icmp_seq=4 ttl=63 time=32.8 ms --- pamdev05.opap.local ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3004ms rtt min/avg/max/mdev = 29.503/31.433/32.931/1.469 ms ","summary":"Unresolvable host workaround","title":"resolved.conf"},{"RelPermalink":"/posts/alfacycling/","content":"Introduction Building a blog from the ground up can be quite interesting. It won’t and can’t be a match to a feature-rich blogging engine, without any substantial effort at least, but it will help you understand topics like REST, CRUD, and many more. A while ago I created alfacycling.com, a primitive blog to get my hands dirty with new technologies.\nArchitecture The idea behind this project was pretty simple, have a landing page with some info about the team and two sub-routes with all the blogging and user functions. There is no user support besides a moderator, with some basic capabilities like creating, editing, or deleting a blog post.\nThis is a monolithic application design, with a MYSQL database for user authentication and data storage. The project is built on Node.js with the help of Express for the Backend and EJS for the Frontend. Some cool libraries were used, like Passport for authenticating users or Puppeteer and Mocha for testing.\nREST Routing Each route is defined in its file. Take for example the /blogs route, where all CRUD operations regarding blogs exist. Defining a route is simple enough with Express.js.\nrouter.get(\u0026#39;/blogs/:id\u0026#39;, async (req, res) =\u0026gt; { res.render(\u0026#39;blogs/show\u0026#39;, { blog: await getOneFromDatabase(req.params.id), blog_id: req.params.id }); }); The GET route in the snippet above returns a single blog entry. Since we are using EJS we have to render each view and pass in, if any, required parameters.\nAbstracting actions Since a lot of code is duplicated, for example a database query, it makes sense to abstract that to a separate file and export it as a module.\nmodule.exports = { getOneFromDatabase: async (id) =\u0026gt; { const sql = \u0026#39;SELECT * FROM blogs WHERE blogs.id = ?;\u0026#39;; const foundBlog = await db.query(sql, id); return foundBlog; }, // More actions }; The action getOneFromDatabase returns a single blog entry that matches a given id. A lot cleaner now!\nAuthentication For authentication Passport was used, a middleware that supports various authentication strategies from local to Facebook and Twitter. Take a look at the local login strategy.\npassport.use(\u0026#39;login\u0026#39;, new LocalStrategy({ usernameField: \u0026#39;username\u0026#39;, passwordField: \u0026#39;password\u0026#39; }, async (username, password, done) =\u0026gt; { const user = await findUser(username); bcrypt.compare(password, user[0].password, (err, result) =\u0026gt; { if (err) throw err; if (result) { return done(null, user[0]); } else { const errors = [{value: \u0026#39;Exists\u0026#39;, msg: \u0026#39;Wrong Password!\u0026#39;, param: \u0026#39;password\u0026#39;, location: \u0026#39;passport\u0026#39;}]; return done(null, false, errors); } }); })); A user is retrieved by his username and has the password he provided compared with the one saved in the database. The comparison is done with the bcrypt library as passwords are not saved in plain text but rather hashed and salted.\nTesting Testing is an integral part of any application, or to phrase it better, it should be. Since our application requires signing in and we want each testing session to be clear of any remains, we have to create a user and session factory.\nmodule.exports = async () =\u0026gt; { const id = crypto.randomBytes(4).toString(\u0026#34;hex\u0026#34;); const sql = \u0026#39;INSERT INTO users (id) values (?)\u0026#39;; await db.query(sql, id); return id; } We firstly create a user entry in the database. That is simple enough, we create an entry with just an id - the only field we are going to need.\nmodule.exports = (id) =\u0026gt; { const sessionObject = { passport: { user: id } }; const session = buffer.from(JSON.stringify(sessionObject)).toString(\u0026#39;base64\u0026#39;); const sig = keygrip.sign(\u0026#39;express:sess=\u0026#39; + session); return {session, sig}; }; Next, we forge a session by creating two cookies: express:sess and express:sess.sig. This will trick our server into thinking we are an authenticated user and thus allowing us to see restricted routes. The cookies name are specific to the cookie middleware we are using.\nDeployment Deployment is handled by docker and docker-compose. The dockerfile for alfacycling.com is pretty straightforward. Pull the specified node image, set the working directory, copy the package.json and package-lock.json files in the container, run npm install, expose the server port, and run the application!\nFROM node:14.16.0-alpine3.10 WORKDIR /usr/src/app/alfacycling.com COPY package*.json ./ RUN npm install EXPOSE 8080 CMD [ \u0026#34;npm\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;dev\u0026#34; ] In docker-compose.yml we define two services: web which is the server behind alfacycling.com, and mysql a database instance. This file is pretty explanatory, you can learn more about each key in the docker-compose documentation. One interesting part is the volumes key, which mounts the specified host directories inside the container, allowing you to make changes on the fly, without having you rebuild the image.\nservices: web: build: . container_name: \u0026#34;alfacycling.com\u0026#34; ports: - \u0026#34;8080:8080\u0026#34; volumes: - ./:/usr/src/app/alfacycling.com - /usr/src/app/alfacycling.com/node_modules depends_on: - mysql mysql: image: mysql container_name: \u0026#34;mysql\u0026#34; restart: always env_file: - mysql-variables.env Conclusion There is a lot more going on in the repository, I analyzed just a few key points of this project, what I thought might be more interesting.\n","summary":"An early exploration of the web front","title":"A naive approach to blogging"},{"RelPermalink":"/posts/ichnaea/","content":"Introduction A while ago I submitted my thesis with its subject being the study of the architecture of BAN and PAN, and their applications, with the aim of optimizing athletes performance, by enhancing personalized training practices. I was also tasked with creating an e-sport application that would collect and analyze data on the athlete’s body position in real-time. I named the e-sport application Icnhaea, and I will be referring to it as so for the rest of this post.\nDeveloping Ichnaea proved a challenge for me, as it tested my knowledge and understanding of full-stack applications. I wanted to build a resilient, multi-tenant application with scaling capabilities that would not just tick the boxes for a thesis but be a worthwhile project that would make me a better engineer.\nThis post will be a rough overview of the thesis. You can find the source and doc here.\nArchitecture To begin with, deployment is handled by docker and docker-compose. Abstracting the configuration to a handy yaml file, while keeping the development environment the same across machines makes deployment a breeze. Running the project will create the following services:\nClient, a Node.js app that collects and sends the data to the backend. Backend, a Node.js server that is responsible for data and user storage, client and SPA socket connection, and exposing an API. Three Databases, each for its distinct purpose: Influxdb, used to store the bulk of data produced from the client devices. A Grafana instance, attached to Influx. Redis, used for publish-subscribe. MongoDB, used as the user store. Frontend, a SPA built on Vue with Typescript. As you can see, I decided to go full-on with JavaScript and TypeScript for Ichnaea, as I was already familiar with lots of libraries and frameworks in the respective ecosystems.\nClient The client device is an Arduino with an IMU sensor. The program can either run on the host machine or on a Raspberry Pi that acts like a getaway for the Arduino. The client does some basic calculations with the help of Johnny-five, then the data is then streamed to the server, and subsequently to the frontend. The frontend does the final calculations that are needed for the model visualization. This way we avoid making any \u0026lsquo;heavy\u0026rsquo; computations on the client device, thus allowing for small and power efficient getaways like a Pi Zero with multiple micro-controllers attached on it.\nWhat is an IMU and how do you make sense of its readings? An Inertial Measurement Unit can be found pretty much everywhere today and is a combination of accelerometers, gyroscopes, and magnetometers. They are used in navigation systems, smartphones, fitness trackers, and many more. Classified by Degrees Of Freedom, an IMU of 9-DOF will feature 3 degrees each of acceleration, magnetic orientation, and angular velocity. As a rule of thumb the higher the DOF rating the more accurate an IMU will be.\nTo get some meaningful readings from an IMU a filter has to be used. There are many implementations from the notoriously hard Kalman filter to the relatively newly founded Madgwick filter. I decided to go with the complementary filter, an easy-to-understand and even easier to implement a filter. With the complementary filter we are keeping the best attributes from each sensor, using the gyro for short-term calculations and the accelerometer for long-term calculations. Why is that?\nThe accelerometer picks up even the smallest forces that are working on the object. If you observe the raw output of an accelerometer you will notice that even the tiniest disturbance is measured. Because the accelerometer will not drift we are going to use a low-pass filter on it.\nGyros on the other hand are not susceptible to external forces and can accurately measure angular velocity. Unfortunately, gyroscopes have a tenancy to drift, making them less accurate over time. For this reason, we are going to use a high-pass filter.\nangle = a * (angle + gyroscopeData * dt) + (1 - a) * accelerometerData Variable a ranges from 95 to 98 percent. You should experiment a bit and choose a suitable percentage for your application. This filter is also very easy on resources, so it\u0026rsquo;s a true fit for low-powered gateways. The client application was designed with extensibility in mind, meaning you can define a new file under the actions directory to handle another type of sensor.\nfunction parseData(imu) { const {temperature, accelerometer, gyro} = imu; // Get pitch, roll, yaw from gyro pitch += (gyro.rate.x / gyroSens) * samplingInterval; roll -= (gyro.rate.y / gyroSens) * samplingInterval; yaw += (gyro.rate.z / gyroSens) * samplingInterval; // Only use accelerometer when forces are ~1g if (accelerometer.acceleration \u0026gt; -1 \u0026amp;\u0026amp; accelerometer.acceleration \u0026lt; 2) { pitch = 0.98 * pitch + 0.02 * accelerometer.pitch; roll = 0.98 * roll + 0.02 * accelerometer.roll; } // Filter out noise (a small tremor appears with too many fraction digits) pitch = toFixed(pitch); roll = toFixed(roll); yaw = toFixed(yaw); return { pointName: \u0026#39;IMU\u0026#39;, uuid: id, temperature: temperature.celsius, pitch, roll, yaw, acceleration: imu.accelerometer.acceleration, inclination: imu.accelerometer.inclination, orientation: imu.accelerometer.orientation, } } I\u0026rsquo;ve set the sampling frequency to 10HZ, just enough to get frequent updates on the body position without bogging down the gateway. Things are pretty straightforward from this point on. The client establishes a socket connection with the backend and synchronously transfers data.\nThere is one got ya here. I am using a 6-DOF IMU and that means there is no reference point for the z-axis. Since I am relying completely on the gyro for the yaw angle, a drift is introduced over time. The orange line in the following chart depicts the slow but constant drift of the yaw angle. Here is where a magnetometer would come in handy.\nThere is a lot more going on here. Check out Euler angles (pitch, roll, yaw), accelerometers, gyroscopes, magnetometers.\nBackend The server for Ichnaea was built on Node.js while making use of many popular packages like celebrate for input validation. Express was used for the creation of the API. A route definition is as simple as stating the HTTP action with the desired endpoint and then handling the request accordingly. I am not going to dive into route implementation details. You can find more in the Express documentation.\nIchnaea was built with consideration for multiple tenants and isolation features. It wouldn\u0026rsquo;t be very smart to stream an athlete\u0026rsquo;s data to a trainer but his own. Besides offering session capabilities with distinct trainer accounts, an adoption concept has been introduced to solve this exact problem. An athlete in the orphan state has no trainer and is open to be adopted. You can then adopt him by entering the unique id (generated in the output of the client device) to the respective field in the frontend. After adopting the client, the data generated is only streamed to the intended recipient.\nmodule.exports = (server) =\u0026gt; { const io = socket(server); io.adapter(redisAdapter({host: \u0026#39;redis\u0026#39;, port: 6379})); io.on(\u0026#39;connection\u0026#39;, socket =\u0026gt; { let client; console.log(`Client with id: ${socket.id} just connected with ${socket.conn.transport.name}!`); socket.on(\u0026#39;disconnect\u0026#39;, () =\u0026gt; console.log(\u0026#39;Client disconnected!\u0026#39;)); socket.conn.on(\u0026#39;upgrade\u0026#39;, () =\u0026gt; console.log(`Client with id: ${socket.id} upgraded to ${socket.conn.transport.name}!`)); socket.on(\u0026#39;subscribe\u0026#39;, async room =\u0026gt; { const {subscribe, id} = JSON.parse(room); const socketID = socket.id.toString(); socket.join(room); console.log(`Client with id: ${socket.id} joined room \u0026#34;${subscribe}\u0026#34;`); try { if (subscribe === \u0026#39;clients\u0026#39;) { client = await Athlete.findOne({id}).populate(\u0026#39;_trainer\u0026#39;); sub.on(\u0026#39;message\u0026#39;, async (channel, msg) =\u0026gt; { if (String(client?._trainer?._id) === JSON.parse(msg)) client._trainer = await User.findOne({_id: client._trainer}); }); sub.subscribe(\u0026#39;updateSocketID\u0026#39;); if (client) { await Athlete.findOneAndUpdate({id}, {socketID}); return; } await saveAthlete(id, socketID); } else if (subscribe === \u0026#39;dashboard\u0026#39;) { client = await User.findOne({id}); pub.publish(\u0026#39;updateSocketID\u0026#39;, JSON.stringify(client._id)); await User.findOneAndUpdate({id}, {socketID}); } } catch (e) { console.log(e); } }); socket.on(\u0026#39;data\u0026#39;, async data =\u0026gt; { if (client?._trainer) { iWrite(data); io.volatile.to(client._trainer.socketID).emit(\u0026#39;console\u0026#39;, data); } }); }); } The aforementioned behavior is demonstrated in the code snippet above. One nasty bug that bothered me for a couple of days was the client device sending data to an old socket id if the dashboard (frontend application) established a socket connection after the client or if the trainer refreshed the page. That happened because the socket id of the dashboard was now outdated since a new socket connection was established and a new id was generated. In addition to code being block scoped in a socket.io event that made it impossible to solve with the libraries built-in functions. Hitting the database at preset intervals was a big no. Imagine having tens of clients doing the same thing. That quickly brings the requests to thousands per minute.\nRedis pub-sub came to the rescue! By subscribing to the updateSocketID event whenever a client connects and triggering it when a dashboard (re)connects, excess database reads are avoided. The only thing the subscriber does on that event trigger is check if his trainer changed id and pick up the new one from the store if so.\nThe rest of the server is pretty basic. Field validation with celebrate, session and user management with bcrypt/passport/cookie-session, cors and rate limiter for security reasons, and the list goes on. Check the repo for more details.\nDatabases Ichnaea utilizes three databases, each for its distinct purpose.\nMongoDB MongoDB was chosen for the user store. Two models were created, one for the user logging to the dashboard and one to represent each distinct athlete. Each model has some basic identification features and the all-important socketID. This field stores the last socket connection id, useful to stream data only to the intended recipient.\nAnother important field is the _trainer, found only in the athletes model. With that we keep a reference to the trainer of the athlete, that is the user _id. To retrieve the trainer details at the same time as the athlete\u0026rsquo;s details, we can use the .populate() function like so.\nclient = await Athlete.findOne({id}).populate(\u0026#39;_trainer\u0026#39;); InfluxDB A time-series database was a precise fit for the serial data generated from the sensors. Writing data to influx is as easy as defining a new Point with the data you want to save. InfluxDB comes with a rich dashboard with various data display capabilities. A Grafana instance is attached to the Influx instance, so you can squeeze out every metric from Ichnaea.\nconst imuPoint = new Point(pointName) .tag(\u0026#39;client\u0026#39;, uuid) .tag(\u0026#39;sensor\u0026#39;, \u0026#39;IMU\u0026#39;) .floatField(\u0026#39;temperature\u0026#39;, temperature) .floatField(\u0026#39;pitch\u0026#39;, pitch) .floatField(\u0026#39;roll\u0026#39;, roll) .floatField(\u0026#39;yaw\u0026#39;, yaw) .floatField(\u0026#39;acceleration\u0026#39;, acceleration) .floatField(\u0026#39;inclination\u0026#39;, inclination) .floatField(\u0026#39;orientation\u0026#39;, orientation); writeApi.writePoint(imuPoint); Redis Redis is used as a publish-subscribe mechanism in two places. The first place we make use of pubsub is updating the socket id dynamically when the dashboard reconnects, as mentioned a couple of sections before. Secondly, it is used as a socket.io adapter, helping when scaling up with multiple instances of the backend. This way we can broadcast a message to multiple clients even if they are connected to a different server. You can read more about that here.\nFrontend Last but not least, the Frontend is built on Vue.js, and is a single page application that makes the dashboard of Ichnaea. Vue is a lovely framework that is extensible with things like routing and state management. I won\u0026rsquo;t go into any details about the framework - feel free to browse the docs.\nI tried to make the dashboard as realistic as possible. It has the following features:\nView the latest README from Github right in the app. Make changes to the user and athlete profile. Adopt or drop athletes. View, search, or inspect athletes and their details. View athlete data in a table or model in real-time. Not something overly impressive, but some basic functionality that would form the building blocks of a real-world app.\nThe main point of the dashboard is the model animation. I fixed a Mixamo model to a Three.js scene, and with the help of quaternions, the model animation is a breeze. I used the build in function from Three.js for the conversion from Euler, setFromEuler(). Taking a look inside setFromEuler(), we can see that the conversion is simple enough.\nconst cos = Math.cos; const sin = Math.sin; const c1 = cos(x / 2); const c2 = cos(y / 2); const c3 = cos(z / 2); const s1 = sin(x / 2); const s2 = sin(y / 2); const s3 = sin(z / 2); this._x = s1 * c2 * c3 + c1 * s2 * s3; this._y = c1 * s2 * c3 - s1 * c2 * s3; this._z = c1 * c2 * s3 + s1 * s2 * c3; this._w = c1 * c2 * c3 - s1 * s2 * s3; I also keep a reference of body parts, and set its position as so.\nthis.head = object.getObjectByName(\u0026#39;mixamorig1Head\u0026#39;); this.head.quaternion.setFromEuler(new Euler(this.roll, this.yaw, this.pitch)); Demo Placing the micro-controller on top of the head and making three discreet movements from neutral to left, back, and upwards right, maps with precision the physical movement and accurately depicts it on the model while testing all three axes, roll-pitch-yaw.\nLet\u0026rsquo;s try the same but for another body part, this time the left arm. Starting from a relaxed hanging position we pull the arm upwards making a slow circular motion and then returning to the starting point. This time due to us deliberately making a slow movement, we can now see the drift introduced from the gyroscope in the yaw angle. That skews with all the readings and thus placing the hand in a wrong final position.\nConclusion This was an overview of Ichnaea, what I thought were the most interesting points, and by no means the full picture. What needs to be improved? Abstracting the sensor positioning logic to a UI where the user can drag and drop each sensor to the respective body and not hard-coding it. Making the client device smaller and self-sufficient with a battery and extensive connectivity options. Lastly, some ML with smart alerts and corrections would provide smart insights.\n","summary":"Scalable IoT solution for real-time body position tracking","title":"Body position tracking"}]