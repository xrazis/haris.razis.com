[{"content":"Introduction The Canvas API is used to draw graphics with Javascript. It can be utilized to make browser games, animations, visualize data, and much more. We will create a really basic Canvas library that receives input from a configuration file and draws the falling items animation.\nParticle Class The Particle class represents a single falling object. It is initialized with the same config object as the Effect class, as it needs many of the same parameters for correctly placing a particle inside the viewport. The constructor is where the majority of the logic is placed. After randomly picking the position in the canvas, we set the size and speed with Math.random() so we get an effect of depth and different weight for each object.\nthis.x = Math.random() * this.config.canvasWidth; this.y = Math.random() * this.config.canvasHeight; this.size = Math.random() * 60 + 20; this.speed = Math.random() * 0.5 + 0.2; Then we need to randomly select an object from the sprite sheet and set the correct width/height. In order to reduce the initial config options required, we set the frame size for the X and Y axis by assuming the elements in the sprite are equally distanced on both axis.\nthis.frameX = Math.floor(Math.random() * this.config.spriteElementsX); this.frameY = Math.floor(Math.random() * this.config.spriteElementsY); this.frameSizeX = this.config.image.width / this.config.spriteElementsX; this.frameSizeY = this.config.image.height / this.config.spriteElementsY; The draw function is responsible for drawing the image on the canvas. The input parameters are best explained on MDN.\nfunction draw(ctx) { ctx.drawImage(this.config.image, this.frameX * this.frameSizeX, this.frameY * this.frameSizeY, this.frameSizeX, this.frameSizeY, this.x, this.y, this.size, this.size); } Lastly, the update function moves the particle down the Y axis, thus providing the snowfall animation. The speed config option determines the speed of the particle falling. It is important to reset the particle position to the top of the canvas once it has surpassed the canvas height - that way the animation will continue running indefinitely.\nfunction update() { this.y += this.speed; if (this.y - this.size \u0026gt; this.config.canvasHeight) { this.y = 0 - this.size; } } Click for the complete Particle class class Particle { constructor(config) { this.config = config; this.x = Math.random() * this.config.canvasWidth; this.y = Math.random() * this.config.canvasHeight; this.size = Math.random() * 60 + 20; this.speed = Math.random() * 0.5 + 0.2; this.frameX = Math.floor(Math.random() * this.config.spriteElementsX); this.frameY = Math.floor(Math.random() * this.config.spriteElementsY); this.frameSizeX = this.config.image.width / this.config.spriteElementsX; this.frameSizeY = this.config.image.height / this.config.spriteElementsY; } get canvasWidth() { return this.config.canvasWidth; } set canvasWidth(width) { this.config.canvasWidth = width; } get canvasHeight() { return this.config.canvasHeight; } set canvasHeight(height) { this.config.canvasHeight = height; } update() { this.y += this.speed; if (this.y - this.size \u0026gt; this.config.canvasHeight) { this.y = 0 - this.size; } } draw(ctx) { ctx.drawImage(this.config.image, this.frameX * this.frameSizeX, this.frameY * this.frameSizeY, this.frameSizeX, this.frameSizeY, this.x, this.y, this.size, this.size); } } Effect Class The Effect class is used to create and manipulate Particle classes. In the constructor the config object is set, then the sprite is loaded, and only when loaded is the rest of the code allowed to run. If you try to execute any code that needs the image prior to that you are not guaranteed that the image will be loaded.\nthis.config = config; this.image = new Image(); this.image.onload = () =\u0026gt; { this.config.image = this.image; this.canvas = document.getElementById(this.config.canvas); this.ctx = this.canvas.getContext(\u0026#39;2d\u0026#39;); this.particlesArray = []; this.setCanvasDimensions(); this.initParticlesArray(); this.handleParticles(); this.startAnimation(); this.registerListeners(); } this.image.src = this.config.spriteUrl; Once the image is loaded, the canvas is created, and a drawing context is set - a 2d rendering context in our case. Then we set the window dimensions both in the canvas and the config object.\nfunction setCanvasDimensions() { this.canvas.width = window.innerWidth; this.canvas.height = window.innerHeight; this.config.canvasWidth = window.innerWidth; this.config.canvasHeight = window.innerHeight; } This is done for a very specific reason. The Particle needs to know the canvas width and height. In this case it is the window width and height, but in other cases it may as well be different. This way we can choose the Particle position randomly somewhere inside that canvas. Keep in mind that we also need to accommodate for any resize events, an expensive but necessary operation.\nfunction setParticleDimensions() { this.particlesArray.forEach(particle =\u0026gt; { particle.canvasWidth = this.config.canvasWidth; particle.canvasHeight = this.config.canvasHeight; }); } function onResize() { this.setCanvasDimensions(); this.setParticleDimensions(); } function registerListeners() { window.addEventListener(\u0026#39;resize\u0026#39;, () =\u0026gt; this.onResize()); } Then, the Particles are created, and the animation is started. I have used the requestAnimationFrame and not a setInterval. That is because setting an interval is not a reliable way to animate things - in the case the function provided takes longer than 16ms (1/60 sec - 60FPS) that will cause blocking and result in dropped frames. The requestAnimationFrame invokes the callback function at each repaint. This is typically every 1/60th of a second, thus providing the wanted frame rate of 60FPS.\nClick for the complete Effect class class Effect { constructor(config) { this.config = config; this.image = new Image(); this.image.onload = () =\u0026gt; { this.config.image = this.image; this.canvas = document.getElementById(this.config.canvas); this.ctx = this.canvas.getContext(\u0026#39;2d\u0026#39;); this.particlesArray = []; this.setCanvasDimensions(); this.initParticlesArray(); this.handleParticles(); this.startAnimation(); this.registerListeners(); } this.image.src = this.config.spriteUrl; } setCanvasDimensions() { this.canvas.width = window.innerWidth; this.canvas.height = window.innerHeight; this.config.canvasWidth = window.innerWidth; this.config.canvasHeight = window.innerHeight; } setParticleDimensions() { this.particlesArray.forEach(particle =\u0026gt; { particle.canvasWidth = this.config.canvasWidth; particle.canvasHeight = this.config.canvasHeight; }); } initParticlesArray() { for (let i = 0; i \u0026lt; this.config.particlesCount; i++) { this.particlesArray.push(new Particle(this.config)); } } clearRect() { this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height); } handleParticles() { this.clearRect(); this.particlesArray.forEach(particle =\u0026gt; { particle.update(); particle.draw(this.ctx); }); } onResize() { this.setCanvasDimensions(); this.setParticleDimensions(); } startAnimation() { const render = () =\u0026gt; { this.handleParticles(); requestAnimationFrame(render); } render(); } registerListeners() { window.addEventListener(\u0026#39;resize\u0026#39;, (e) =\u0026gt; this.onResize()); } } Initialization of the effect The following script is the initialization code for the Effect class. The input is a config object with the following options:\ncanvas: The id of the Canvas HTML element. spriteUrl: The image (in the form of a sprite) to be used for the falling objects. particlesCount: The items count to be created - they are randomly selected from the sprite. spriteElementsX: The images in the X axis of the sprite sheet. spriteElementsY: The images in the Y axis of the sprite sheet. const config = { canvas: \u0026#39;canvas-animation\u0026#39;, particlesCount: 10, spriteUrl: \u0026#34;https://haris.razis.com/blog/20240114-cat.png\u0026#34;, spriteElementsX: 5, spriteElementsY: 2, }; new Effect(config); You can observe the code running in the following CodePen, voilÃ !\nSee the Pen by Haris Razis (@xrazis) on CodePen. Sources MDN Web Docs - Canvas API ","permalink":"https://haris.razis.com/posts/fundamental-canvas-library/","summary":"Building a basic Canvas library to animate falling objects in the background (snowfall effect)","title":"Creating Falling Objects Animation Using Canvas"},{"content":"Introduction Dotfiles are user-specific application configuration files that your filesystem does not display by default, and they start with a dot (ie ~/.gitconfig). It is common to host them in a git repository in order to keep a consistent environment across machines. We are going to automate some of our application installs and symlink our system dotfiles to a git repository.\nRepository structure and Dotfiles Firstly initialize a git repository, ie my GitHub repo has the following structure.\nA handy linux package is tree, a recursive directory listing program that lists contents in a tree-like format. I\u0026rsquo;ve used it with the option -a which prints all files including dotfiles, and -I which ignores the given directories. The command used for the tree generated bellow is tree -a -I .git.\n. â”œâ”€â”€ git â”‚ â””â”€â”€ .gitconfig â”œâ”€â”€ .idea â”‚ â”œâ”€â”€ dotfiles.iml â”‚ â”œâ”€â”€ .gitignore â”‚ â”œâ”€â”€ modules.xml â”‚ â”œâ”€â”€ vcs.xml â”‚ â””â”€â”€ workspace.xml â”œâ”€â”€ install.sh â”œâ”€â”€ LICENSE â”œâ”€â”€ logiops â”‚ â””â”€â”€ logid.cfg â”œâ”€â”€ pop â”‚ â””â”€â”€ .config â”‚ â””â”€â”€ pop-shell â”‚ â””â”€â”€ config.json â”œâ”€â”€ scripts â”‚ â”œâ”€â”€ apts.sh â”‚ â”œâ”€â”€ directs.sh â”‚ â”œâ”€â”€ flatpaks.sh â”‚ â””â”€â”€ snaps.sh â””â”€â”€ zsh â””â”€â”€ .zshrc At the top level we have the install.sh file which is the entry point of our script, and a directory for each of the configuration files. Let\u0026rsquo;s have a deeper look at install.sh.\n#!/bin/bash if [ ! -f /usr/bin/stow ] then sudo apt-get install stow fi if [ \u0026#34;$1\u0026#34; == \u0026#34;apps\u0026#34; ] then source ./scripts/apts.sh source ./scripts/directs.sh source ./scripts/snaps.sh source ./scripts/flatpaks.sh fi rm ../.zshrc \\ ../.config/pop-shell/config.json \\ ../.gitconfig \\ /etc/logid.cfg stow zsh \\ pop \\ git stow -t /etc logiops echo \u0026#34;Done!\u0026#34; The first line of the script is a shebang - it tells our system what interpreter to use. Afterward we are going to check if stow is installed. Stow is a symlink farm manager that was originally used to manage software packages. Despite that not being the case today - as there are more sophisticated package managers available - it is still being used to manage configuration files in a controlled and organized way, exactly what we need!\nUsing stow is quite simple, just append the configuration file you want to symlink after the command and voilÃ ! If the directory you want to symlink from is not the working dir you can change that with the - t option. The way stow symlinks is quite interesting, be sure to read the man page.\nThe dotfiles are application specific. Copy the respective file and customize it to your needs.\nInstallation automation Installing apps in a fresh system can get tedious, shouldn\u0026rsquo;t we automate much of that with a simple script?\n#!/bin/bash echo \u0026#34;Installing snapd...\u0026#34; apt install -y snapd echo \u0026#34;Installing speedtest...\u0026#34; apt install -y speedtest-cli echo \u0026#34;Installing restic...\u0026#34; apt install -y restic echo \u0026#34;Installing curl...\u0026#34; apt install -y curl echo \u0026#34;Installing httpie\u0026#34; apt install -y httpie echo \u0026#34;Installing termius...\u0026#34; curl -OL https://www.termius.com/download/linux/Termius.deb dpkg -i Termius.deb # ... The applications will be installed serially, just like they would when installing them standalone in the terminal. Make sure to provide any input that an app might need (ie - y for apt packages). You can also test them in a Multipass virtual machine.\nRepository https://github.com/xrazis/dotfiles/ ","permalink":"https://haris.razis.com/posts/dotfiles/","summary":"How to automate your system installs and keep your application configuration in dotfiles","title":"Dotfiles"},{"content":"Introduction At my day job we decided to rewrite a Liferay portlet to React, it displays draws and results data from different numeric games (the client is a betting company slowly porting many of their games online). This particular portlet has been a pain in the ass to maintain, port new CRs, and was horrible performance wise, and that is because:\nThe code was inherited from another company and was written a long time ago with questionable structure/logic. There was never enough time to do a proper cleanup or rewrite. It tries to handle many different numeric games and in turn the code ends up being incomprehensible and bug prone. It has many expensive DOM operations. React Application Developing the React application standalone and outside of Liferay is a huge boost of productivity as you can take advantage of features like live reloading and your IDE\u0026rsquo;s debugger. This particular project is based on a monolithic architecture with no direct communication to the server from the client (ex websockets). It is tightly coupled with third party vendors and much of the information displayed comes from APIs that other teams use as well (ex mobile vendor). Some of the API calls happen at the backend at a predefined interval, this way the endpoint does not get bottled down from requests. That information is exposed to a window variable so that specific context can be accessible from different parts of the application.\nThe majority of data displayed in the portlet are fed in from the aforementioned window variable. On some user actions an endpoint is queried and the data displayed is updated. That is done with a useState hook that lets us add state to our components. The useState hook is particularly useful as React takes care of what to update in the UI - you only have to declaratively program the UI. You should definitely have a look at the docs as state management can get tricky without the right knowledge.\n// Declare the data state const [data, setData] = useState(window.contextContributor); const fetchData = (drawId) =\u0026gt; { fetch(`${apiEndpoint}/draws/v3.0/5104/${drawId}`) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; { // Update the state with the new data. React will take care of updating the UI setData(data); }) .catch((error) =\u0026gt; console.debug(`[DRAWS \u0026amp;\u0026amp; RESULTS DRAWS ERROR]: ${error}`)); }; // ... // Display some repeative nested field of data using Array.prototype.map() \u0026lt;div className=\u0026#39;balls-row\u0026#39;\u0026gt; {data.winningNumbers.list.sort().map(number =\u0026gt; { return (\u0026lt;GameBall ballColor=\u0026#39;blue-ball\u0026#39; number={number} key={number} /\u0026gt;); })} \u0026lt;/div\u0026gt; // Add the click listener \u0026lt;button className=\u0026#39;btn draw-btn-left\u0026#39; onClick={() =\u0026gt; handleDrawClick(data.drawId - 1)}\u0026gt; \u0026lt;/button\u0026gt; Another helpful hook is useEffect which is used to synchronize the component with an external system. In our particular case we are going to use it to populate a html select with data from an API. This select displays the draw id\u0026rsquo;s for a given month. The same action is also triggered when the user selects a different month.\nWhen a month is changed three actions need to take place:\nFind the month in calendar format (1-12) using the monthTuple (see i18next down bellow for month tuple) and Array.prototype.findIndex(). For example, given September the variable monthInDateFormat is going to be 9. Set the new month in state. Correctly format the date for the endpoint, then fetch the new draw id\u0026rsquo;s and update the respective state. const fetchDraws = (monthInDateFormat) =\u0026gt; { const formattedMonth = monthInDateFormat \u0026gt;= 10 ? monthInDateFormat : `0${monthInDateFormat}`; const daysInMonth = new Date(year, monthInDateFormat, 0).getDate(); fetch(`${apiEndpoint}/draws/v3.0/5104/draw-date/${year}-${formattedMonth}-01/${year}-${formattedMonth}-${daysInMonth}/draw-id`) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; setDraws(data)) .catch((error) =\u0026gt; console.debug(`[DRAWS \u0026amp;\u0026amp; RESULTS DRAW DATE ERROR]: ${error}`)); }; const changeMonth = (newMonth) =\u0026gt; { const monthInDateFormat = monthTuple.findIndex(e =\u0026gt; e === newMonth) + 1; setMonth(newMonth); // New draws need to be fetched every time the month changes fetchDraws(monthInDateFormat); }; useEffect(() =\u0026gt; { changeMonth(monthTuple[thisMonth]); }, []); By passing an empty dependency array to the useEffect hook we are telling React that it uses no reactive values, and it can be run after the initial render. This way we won\u0026rsquo;t wait on an API call for a UI element that is not critical to appear on render time.\nAs far as localization goes I\u0026rsquo;ve chosen to use react-i18next for two reasons. Firstly I was already familiar with i18next from some personal projects, and secondly I didn\u0026rsquo;t want to be depended on Liferay\u0026rsquo;s inbuilt localization features because I was developing it as a standalone application. Setting it up is a breeze - I decided to install the base package and the browser-languagedetector. The configuration lives in a standalone file, as do the translations.\nimport i18n from \u0026#39;i18next\u0026#39;; import {initReactI18next} from \u0026#39;react-i18next\u0026#39;; import LanguageDetector from \u0026#39;i18next-browser-languagedetector\u0026#39;; import el from \u0026#39;./locales/el/translation.json\u0026#39; import en from \u0026#39;./locales/en/translation.json\u0026#39; i18n .use(LanguageDetector) .use(initReactI18next) .init({ fallbackLng: \u0026#39;el\u0026#39;, resources: { el: {common: el}, en: {common: en} }, detection: { order: [\u0026#39;htmlTag\u0026#39;], convertDetectedLanguage: (lng) =\u0026gt; lng === \u0026#39;el-GR\u0026#39; ? \u0026#39;el\u0026#39; : \u0026#39;en\u0026#39; } }); export default i18n; It is now ready to be imported to your application, just remember to use the correct namespace like so:\nconst {t} = useTranslation(\u0026#39;common\u0026#39;); // ... \u0026lt;h2 className=\u0026#39;title\u0026#39;\u0026gt;{t(\u0026#39;title\u0026#39;)}\u0026lt;/h2\u0026gt; One neat option you can pass to the translation component is returnObjects, which returns an object. This way we can localize things like dates and avoid using a third party plugin like moment.js. We can take advantage of the javascript Date object and grab the date we want.\n{ \u0026#34;monthTuple\u0026#34;: { \u0026#34;0\u0026#34;: \u0026#34;January\u0026#34;, \u0026#34;1\u0026#34;: \u0026#34;February\u0026#34;, \u0026#34;2\u0026#34;: \u0026#34;March\u0026#34;, \u0026#34;3\u0026#34;: \u0026#34;April\u0026#34;, \u0026#34;4\u0026#34;: \u0026#34;May\u0026#34;, \u0026#34;5\u0026#34;: \u0026#34;June\u0026#34;, \u0026#34;6\u0026#34;: \u0026#34;July\u0026#34;, \u0026#34;7\u0026#34;: \u0026#34;August\u0026#34;, \u0026#34;8\u0026#34;: \u0026#34;September\u0026#34;, \u0026#34;9\u0026#34;: \u0026#34;October\u0026#34;, \u0026#34;10\u0026#34;: \u0026#34;November\u0026#34;, \u0026#34;11\u0026#34;: \u0026#34;December\u0026#34; } } const date = new Date(); const thisMonth = date.getMonth(); const monthTuple = Object.values(t(\u0026#39;monthTuple\u0026#39;, {returnObjects: true})); const [month, setMonth] = useState(monthTuple[thisMonth]); Porting to Liferay There are two ways to create a React widget:\nUse Blade to create a liferay-js module. Use Yeoman generator to create or adapt an existing React application. I found that creating a standalone React application with npx create-react-app and then porting it with yeoman proved useful, as I developed the module much faster outside of Liferay and only made some finishing touches once ported.\nDeploying was definitely not a breeze. We use Jenkins to build and deploy to a remote server but the module would fail and thus ruin the whole pipeline. I found no quick solution to that, so I resulted to rename the build scripts (so they wouldn\u0026rsquo;t get picked up by Gradle), build the module locally, and then deploy it to the remote server.\nConclusion This was the first React application and Liferay widget I had to develop. Despite some early hiccups, especially when trying to figure out how the Liferay widget works, I do believe that developer experience is boosted, performance is improved, and maintenance is made easier - in sort React ðŸ’•.\nSources Big props to my senior for guiding me because I would have probably ended up in an asylum trying to figure out the Liferay toolkit.\nhttps://github.com/0xAnakin/liferay-react-demo https://github.com/0xAnakin/Liferay74u46-react-demo https://help.liferay.com/hc/en-us/articles/360029028051-Developing-a-React-Application https://liferay.dev/en/blogs/-/blogs/liferay-react-portlets https://help.liferay.com/hc/en-us/articles/360028832872-Understanding-the-npmbundlerrc-s-Structure https://help.liferay.com/hc/en-us/articles/360028832892-How-the-Default-Preset-Configures-the-liferay-npm-bundler ","permalink":"https://haris.razis.com/posts/liferay-widgets-react/","summary":"How to build a React Portlet on Liferay 7.2 with widgets","title":"Liferay widgets and React"},{"content":"Introduction Statically generated sites are fast and flexible but don\u0026rsquo;t have the ability to send contact us emails due to the lack of a backend. A simple solution would be using a service like Airform, Mailgun, or Formspree, but where\u0026rsquo;s the fun in that? Instead, we can build a simple Express.js API that will handle POST requests from a form and will in turn forward that email to a mailbox.\nFrontend First of all make sure you have a working form. Create the fields you need, give a descriptive id to all the elements, and omit any action or method attributes.\n\u0026lt;form id=\u0026#34;contactForm\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;mb-6\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;form-label\u0026#34; for=\u0026#34;name\u0026#34;\u0026gt; ÎŒÎ½Î¿Î¼Î± \u0026lt;span class=\u0026#34;text-red-500\u0026#34;\u0026gt;*\u0026lt;/span\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;input class=\u0026#34;form-input\u0026#34; id=\u0026#34;name\u0026#34; name=\u0026#34;name\u0026#34; placeholder=\u0026#34;Frida Kahlo\u0026#34; type=\u0026#34;text\u0026#34;/\u0026gt; \u0026lt;/div\u0026gt; ... \u0026lt;/form\u0026gt; Then for the scripting part we are going to set an Event Listener on the form and listen for any submit events. Make sure to include mode: \u0026quot;cors\u0026quot; as we will be filtering request on the server side based on the origin of the request.\nconst endpoint = \u0026#39;{{ site.Params.contact_form_action }}\u0026#39;; const contactForm = document.getElementById(\u0026#39;contactForm\u0026#39;); contactForm.addEventListener(\u0026#39;submit\u0026#39;, event =\u0026gt; { event.preventDefault(); const data = new FormData(contactForm); data.set(\u0026#39;serverEmail\u0026#39;, \u0026#39;info@antamacollective.gr\u0026#39;); fetch(endpoint, { method: \u0026#34;POST\u0026#34;, mode: \u0026#34;cors\u0026#34;, body: data, }).then(response =\u0026gt; { if (response.status === 200) { document.getElementById(\u0026#39;successful-email\u0026#39;).classList.remove(\u0026#39;hidden\u0026#39;); } else { document.getElementById(\u0026#39;failed-email\u0026#39;).classList.remove(\u0026#39;hidden\u0026#39;); } document.getElementById(\u0026#39;submitterButton\u0026#39;).setAttribute(\u0026#39;disabled\u0026#39;, \u0026#39;\u0026#39;); }); }); Backend The backend is a simple Node.js server with Express.js. One integral part is CORS, which only allows specific domains to access the POST path. In sort, CORS is an HTTP-header based mechanism that allows a server to indicate any origins other than its own from which a browser should permit loading resources. This is simply done in Express with the cors middleware. The configuration is simple enough, as seen in the code snippet bellow, you just have to define the allowed origins. The default configuration is adequate for most cases.\nconst corsOptions = { origin: [\u0026#39;https://antamacollective.gr\u0026#39;], } app.get(\u0026#39;/send-email\u0026#39;, cors(corsOptions), (req, res) =\u0026gt; { res.status(200).send(\u0026#39;This route is only visible from CORS allowed origin.\u0026#39;); }); The /send-email route is responsible for posting the email. There are two middleware, multer which decodes multipart/form-data and cors which was explained in the section above. Within the function the fields are validated against a Joi schema and then the email is sent with Nodemailer.\napp.post(\u0026#39;/send-email\u0026#39;, cors(corsOptions), multer().none(), async (req, res) =\u0026gt; { try { const {name, email, message, serverEmail} = req.body; const host = req.hostname; //Validate against our schema await emailSchema.validateAsync({name, email, message}); const mail = { from: email_user, replyTo: email, to: serverEmail, subject: `Email from ${host} - inquiry from: ${name}`, text: message }; transporter.sendMail(mail, (err, info) =\u0026gt; { if (err) { res.sendStatus(500); return; } res.sendStatus(200); }); } catch (err) { res.sendStatus(400); } }); Repositories https://github.com/xrazis/form-endpoint/ https://github.com/xrazis/antamacollective.gr ","permalink":"https://haris.razis.com/posts/form-endpoint/","summary":"Minimal Express.js server to post emails from static sites build with tools like Hugo","title":"Handling emails for statically generated sites"},{"content":"Introduction Having SSH access to the server you are battling with is an invaluable tool - no matter how feature-rich the console may be. In the few past days, we (my father and I) faced connectivity issues on a corporate network where a specific client (a POS device) would not stay attached to the nearest access point but instead hop to an AP further away. That caused connectivity problems with slow/failed transactions. It finally proved to be a miss-configuration issue (as it always is) but had we had proper logging, we could have resolved the issue much sooner.\nUniFi OS The majority of hardware we use is from Ubiquity, with many different APs of various generations and a UDM PRO tying them all down. Unfortunately, Ubiquity\u0026rsquo;s documentation is sparse, so the information seen in this post was collected from blogs and community forums.\nThe first step is to enable SSH access to the UDM through the settings and set a password. Then SSH root@192.168.1.1 and enter the password you\u0026rsquo;ve just set. UniFi OS is based on Buildroot with some basic Linux commands (see those with help) and some UniFi-specific commands.\nFor some weird reason, UI decided we can\u0026rsquo;t have docs for the CLI so we have to do some digging ourselves. As mentioned on the community forum we can find the commands defined in the file syswrapper.sh. Edit that file with vi:\nvi `whichÂ syswrapper.sh` Then search for case $cmd in in the file. You can search in vi by hitting / and typing the search term. Don\u0026rsquo;t forget you can exit the file with :q!, which quits without saving. There is a thread on Reddit and a repository on GitHub that documents some of these commands. The following is a snippet from that file.\ncase $cmd in set-tmp-ip) exit_if_fake $cmd $* ;; set-adopt) # set-adopt \u0026lt;url\u0026gt; \u0026lt;authkey\u0026gt; mca-ctrl -t connect -s \u0026#34;$1\u0026#34; -k \u0026#34;$2\u0026#34; ;; set-channel) # set-channel \u0026lt;radio\u0026gt; \u0026lt;channel\u0026gt; # FIXME: dual radio for ath in `ls /proc/sys/net/*/%parent | cut -d \u0026#39;/\u0026#39; -f 5`; do iwconfig $ath channel $1 done ;; # ... Extracting logs We are going to extract log files from two locations:\n/var/log/messages - error logs. # Save the errors to a file cat /var/log/messages \u0026gt; messages.log # Then transfer the file to your machine scp messages.log \u0026lt;user\u0026gt;@\u0026lt;device-name\u0026gt;:\u0026lt;path/to/transfer/to\u0026gt; /data/unifi-core/logs - a directory that contains various UniFi OS logs. # Archive the whole dir tar -zcvf unifi-core-logs.tar.gz /data/unifi-core/logs/ # Then transfer the file to your machine scp messages.log \u0026lt;user\u0026gt;@\u0026lt;device-name\u0026gt;:\u0026lt;path/to/transfer/to\u0026gt; From here on, you are going into a rabbit hole. May your soul find peace because reading these logs drove me crazy.\nAttributions UniFi CLI / SSH commands list All UniFi SSH Commands that You Want to Know How To SSH Into Your UniFi Dream Machine UniFi - How to View Log Files UniFi - Getting Support Files and Logs ","permalink":"https://haris.razis.com/posts/debugging-unifi-connections/","summary":"Accessing Unifi OS logs","title":"Debugging Unifi connections"},{"content":"Introduction I encountered a weird issue the other day at work where I could not resolve a specific dev server on my laptop. Despite being logged to the company\u0026rsquo;s VPN and being able to resolve all the other servers, this specific one stubbornly refused. Going through the debugging process of a complex network was not an option, so resolved.conf came to the rescue.\nNetwork Name Resolution configuration files The file resolved.conf is responsible for local DNS and LLNMR (Link-Local Multicast Name Resolution) name resolution. The main file can be found under /etc/systemd and its entries override any defaults. The following configuration was needed for the server to resolve.\nDNS - Define the IPv4/IPv6 entry (or entries) to use as a system DNS servers. Domains - Domains we want to process using the predefined DNS servers. They are processed in the order they are listed, until a match is found. DNS=195.47.208.14 Domains=opap.local open.local After saving the file with the new entry reload the services as such:\n~ âžœ sudo systemctl daemon-reload sudo systemctl restart systemd-networkd sudo systemctl restart systemd-resolved And check that the server now resolves:\n~ âžœ ping tzokerdev.opap.gr -c 4 PING pamdev05.opap.local (10.126.2.45) 56(84) bytes of data. 64 bytes from 10.126.2.45 (10.126.2.45): icmp_seq=1 ttl=63 time=32.9 ms 64 bytes from 10.126.2.45 (10.126.2.45): icmp_seq=2 ttl=63 time=30.5 ms 64 bytes from 10.126.2.45 (10.126.2.45): icmp_seq=3 ttl=63 time=29.5 ms 64 bytes from 10.126.2.45 (10.126.2.45): icmp_seq=4 ttl=63 time=32.8 ms --- pamdev05.opap.local ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3004ms rtt min/avg/max/mdev = 29.503/31.433/32.931/1.469 ms ","permalink":"https://haris.razis.com/posts/resolved.conf/","summary":"Unresolvable host workaround","title":"resolved.conf"},{"content":"Introduction Building a blog from the ground up can be quite interesting. It wonâ€™t and canâ€™t be a match to a feature-rich blogging engine, without any substantial effort at least, but it will help you understand topics like REST, CRUD, and many more. A while ago I created alfacycling.com, a primitive blog to get my hands dirty with new technologies.\nArchitecture The idea behind this project was pretty simple, have a landing page with some info about the team and two sub-routes with all the blogging and user functions. There is no user support besides a moderator, with some basic capabilities like creating, editing, or deleting a blog post.\nThis is a monolithic application design, with a MYSQL database for user authentication and data storage. The project is built on Node.js with the help of Express for the Backend and EJS for the Frontend. Some cool libraries were used, like Passport for authenticating users or Puppeteer and Mocha for testing.\nREST Routing Each route is defined in its file. Take for example the /blogs route, where all CRUD operations regarding blogs exist. Defining a route is simple enough with Express.js.\nrouter.get(\u0026#39;/blogs/:id\u0026#39;, async (req, res) =\u0026gt; { res.render(\u0026#39;blogs/show\u0026#39;, { blog: await getOneFromDatabase(req.params.id), blog_id: req.params.id }); }); The GET route in the snippet above returns a single blog entry. Since we are using EJS we have to render each view and pass in, if any, required parameters.\nAbstracting actions Since a lot of code is duplicated, for example a database query, it makes sense to abstract that to a separate file and export it as a module.\nmodule.exports = { getOneFromDatabase: async (id) =\u0026gt; { const sql = \u0026#39;SELECT * FROM blogs WHERE blogs.id = ?;\u0026#39;; const foundBlog = await db.query(sql, id); return foundBlog; }, // More actions }; The action getOneFromDatabase returns a single blog entry that matches a given id. A lot cleaner now!\nAuthentication For authentication Passport was used, a middleware that supports various authentication strategies from local to Facebook and Twitter. Take a look at the local login strategy.\npassport.use(\u0026#39;login\u0026#39;, new LocalStrategy({ usernameField: \u0026#39;username\u0026#39;, passwordField: \u0026#39;password\u0026#39; }, async (username, password, done) =\u0026gt; { const user = await findUser(username); bcrypt.compare(password, user[0].password, (err, result) =\u0026gt; { if (err) throw err; if (result) { return done(null, user[0]); } else { const errors = [{value: \u0026#39;Exists\u0026#39;, msg: \u0026#39;Wrong Password!\u0026#39;, param: \u0026#39;password\u0026#39;, location: \u0026#39;passport\u0026#39;}]; return done(null, false, errors); } }); })); A user is retrieved by his username and has the password he provided compared with the one saved in the database. The comparison is done with the bcrypt library as passwords are not saved in plain text but rather hashed and salted.\nTesting Testing is an integral part of any application, or to phrase it better, it should be. Since our application requires signing in and we want each testing session to be clear of any remains, we have to create a user and session factory.\nmodule.exports = async () =\u0026gt; { const id = crypto.randomBytes(4).toString(\u0026#34;hex\u0026#34;); const sql = \u0026#39;INSERT INTO users (id) values (?)\u0026#39;; await db.query(sql, id); return id; } We firstly create a user entry in the database. That is simple enough, we create an entry with just an id - the only field we are going to need.\nmodule.exports = (id) =\u0026gt; { const sessionObject = { passport: { user: id } }; const session = buffer.from(JSON.stringify(sessionObject)).toString(\u0026#39;base64\u0026#39;); const sig = keygrip.sign(\u0026#39;express:sess=\u0026#39; + session); return {session, sig}; }; Next, we forge a session by creating two cookies: express:sess and express:sess.sig. This will trick our server into thinking we are an authenticated user and thus allowing us to see restricted routes. The cookies name are specific to the cookie middleware we are using.\nDeployment Deployment is handled by docker and docker-compose. The dockerfile for alfacycling.com is pretty straightforward. Pull the specified node image, set the working directory, copy the package.json and package-lock.json files in the container, run npm install, expose the server port, and run the application!\nFROM node:14.16.0-alpine3.10 WORKDIR /usr/src/app/alfacycling.com COPY package*.json ./ RUN npm install EXPOSE 8080 CMD [ \u0026#34;npm\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;dev\u0026#34; ] In docker-compose.yml we define two services: web which is the server behind alfacycling.com, and mysql a database instance. This file is pretty explanatory, you can learn more about each key in the docker-compose documentation. One interesting part is the volumes key, which mounts the specified host directories inside the container, allowing you to make changes on the fly, without having you rebuild the image.\nservices: web: build: . container_name: \u0026#34;alfacycling.com\u0026#34; ports: - \u0026#34;8080:8080\u0026#34; volumes: - ./:/usr/src/app/alfacycling.com - /usr/src/app/alfacycling.com/node_modules depends_on: - mysql mysql: image: mysql container_name: \u0026#34;mysql\u0026#34; restart: always env_file: - mysql-variables.env Conclusion There is a lot more going on in the repository, I analyzed just a few key points of this project, what I thought might be more interesting.\n","permalink":"https://haris.razis.com/posts/alfacycling/","summary":"An early exploration of the web front","title":"A naive approach to blogging"},{"content":"Introduction A while ago I submitted my thesis with its subject being the study of the architecture of BAN and PAN, and their applications, with the aim of optimizing athletes performance, by enhancing personalized training practices. I was also tasked with creating an e-sport application that would collect and analyze data on the athleteâ€™s body position in real-time. I named the e-sport application Icnhaea, and I will be referring to it as so for the rest of this post.\nDeveloping Ichnaea proved a challenge for me, as it tested my knowledge and understanding of full-stack applications. I wanted to build a resilient, multi-tenant application with scaling capabilities that would not just tick the boxes for a thesis but be a worthwhile project that would make me a better engineer.\nThis post will be a rough overview of the thesis. You can find the source and doc here.\nArchitecture To begin with, deployment is handled by docker and docker-compose. Abstracting the configuration to a handy yaml file, while keeping the development environment the same across machines makes deployment a breeze. Running the project will create the following services:\nClient, a Node.js app that collects and sends the data to the backend. Backend, a Node.js server that is responsible for data and user storage, client and SPA socket connection, and exposing an API. Three Databases, each for its distinct purpose: Influxdb, used to store the bulk of data produced from the client devices. A Grafana instance, attached to Influx. Redis, used for publish-subscribe. MongoDB, used as the user store. Frontend, a SPA built on Vue with Typescript. As you can see, I decided to go full-on with JavaScript and TypeScript for Ichnaea, as I was already familiar with lots of libraries and frameworks in the respective ecosystems.\nClient The client device is an Arduino with an IMU sensor. The program can either run on the host machine or on a Raspberry Pi that acts like a getaway for the Arduino. The client does some basic calculations with the help of Johnny-five, then the data is then streamed to the server, and subsequently to the frontend. The frontend does the final calculations that are needed for the model visualization. This way we avoid making any \u0026lsquo;heavy\u0026rsquo; computations on the client device, thus allowing for small and power efficient getaways like a Pi Zero with multiple micro-controllers attached on it.\nWhat is an IMU and how do you make sense of its readings? An Inertial Measurement Unit can be found pretty much everywhere today and is a combination of accelerometers, gyroscopes, and magnetometers. They are used in navigation systems, smartphones, fitness trackers, and many more. Classified by Degrees Of Freedom, an IMU of 9-DOF will feature 3 degrees each of acceleration, magnetic orientation, and angular velocity. As a rule of thumb the higher the DOF rating the more accurate an IMU will be.\nTo get some meaningful readings from an IMU a filter has to be used. There are many implementations from the notoriously hard Kalman filter to the relatively newly founded Madgwick filter. I decided to go with the complementary filter, an easy-to-understand and even easier to implement a filter. With the complementary filter we are keeping the best attributes from each sensor, using the gyro for short-term calculations and the accelerometer for long-term calculations. Why is that?\nThe accelerometer picks up even the smallest forces that are working on the object. If you observe the raw output of an accelerometer you will notice that even the tiniest disturbance is measured. Because the accelerometer will not drift we are going to use a low-pass filter on it.\nGyros on the other hand are not susceptible to external forces and can accurately measure angular velocity. Unfortunately, gyroscopes have a tenancy to drift, making them less accurate over time. For this reason, we are going to use a high-pass filter.\nangle = a * (angle + gyroscopeData * dt) + (1 - a) * accelerometerData Variable a ranges from 95 to 98 percent. You should experiment a bit and choose a suitable percentage for your application. This filter is also very easy on resources, so it\u0026rsquo;s a true fit for low-powered gateways. The client application was designed with extensibility in mind, meaning you can define a new file under the actions directory to handle another type of sensor.\nfunction parseData(imu) { const {temperature, accelerometer, gyro} = imu; // Get pitch, roll, yaw from gyro pitch += (gyro.rate.x / gyroSens) * samplingInterval; roll -= (gyro.rate.y / gyroSens) * samplingInterval; yaw += (gyro.rate.z / gyroSens) * samplingInterval; // Only use accelerometer when forces are ~1g if (accelerometer.acceleration \u0026gt; -1 \u0026amp;\u0026amp; accelerometer.acceleration \u0026lt; 2) { pitch = 0.98 * pitch + 0.02 * accelerometer.pitch; roll = 0.98 * roll + 0.02 * accelerometer.roll; } // Filter out noise (a small tremor appears with too many fraction digits) pitch = toFixed(pitch); roll = toFixed(roll); yaw = toFixed(yaw); return { pointName: \u0026#39;IMU\u0026#39;, uuid: id, temperature: temperature.celsius, pitch, roll, yaw, acceleration: imu.accelerometer.acceleration, inclination: imu.accelerometer.inclination, orientation: imu.accelerometer.orientation, } } I\u0026rsquo;ve set the sampling frequency to 10HZ, just enough to get frequent updates on the body position without bogging down the gateway. Things are pretty straightforward from this point on. The client establishes a socket connection with the backend and synchronously transfers data.\nThere is one got ya here. I am using a 6-DOF IMU and that means there is no reference point for the z-axis. Since I am relying completely on the gyro for the yaw angle, a drift is introduced over time. The orange line in the following chart depicts the slow but constant drift of the yaw angle. Here is where a magnetometer would come in handy.\nThere is a lot more going on here. Check out Euler angles (pitch, roll, yaw), accelerometers, gyroscopes, magnetometers.\nBackend The server for Ichnaea was built on Node.js while making use of many popular packages like celebrate for input validation. Express was used for the creation of the API. A route definition is as simple as stating the HTTP action with the desired endpoint and then handling the request accordingly. I am not going to dive into route implementation details. You can find more in the Express documentation.\nIchnaea was built with consideration for multiple tenants and isolation features. It wouldn\u0026rsquo;t be very smart to stream an athlete\u0026rsquo;s data to a trainer but his own. Besides offering session capabilities with distinct trainer accounts, an adoption concept has been introduced to solve this exact problem. An athlete in the orphan state has no trainer and is open to be adopted. You can then adopt him by entering the unique id (generated in the output of the client device) to the respective field in the frontend. After adopting the client, the data generated is only streamed to the intended recipient.\nmodule.exports = (server) =\u0026gt; { const io = socket(server); io.adapter(redisAdapter({host: \u0026#39;redis\u0026#39;, port: 6379})); io.on(\u0026#39;connection\u0026#39;, socket =\u0026gt; { let client; console.log(`Client with id: ${socket.id} just connected with ${socket.conn.transport.name}!`); socket.on(\u0026#39;disconnect\u0026#39;, () =\u0026gt; console.log(\u0026#39;Client disconnected!\u0026#39;)); socket.conn.on(\u0026#39;upgrade\u0026#39;, () =\u0026gt; console.log(`Client with id: ${socket.id} upgraded to ${socket.conn.transport.name}!`)); socket.on(\u0026#39;subscribe\u0026#39;, async room =\u0026gt; { const {subscribe, id} = JSON.parse(room); const socketID = socket.id.toString(); socket.join(room); console.log(`Client with id: ${socket.id} joined room \u0026#34;${subscribe}\u0026#34;`); try { if (subscribe === \u0026#39;clients\u0026#39;) { client = await Athlete.findOne({id}).populate(\u0026#39;_trainer\u0026#39;); sub.on(\u0026#39;message\u0026#39;, async (channel, msg) =\u0026gt; { if (String(client?._trainer?._id) === JSON.parse(msg)) client._trainer = await User.findOne({_id: client._trainer}); }); sub.subscribe(\u0026#39;updateSocketID\u0026#39;); if (client) { await Athlete.findOneAndUpdate({id}, {socketID}); return; } await saveAthlete(id, socketID); } else if (subscribe === \u0026#39;dashboard\u0026#39;) { client = await User.findOne({id}); pub.publish(\u0026#39;updateSocketID\u0026#39;, JSON.stringify(client._id)); await User.findOneAndUpdate({id}, {socketID}); } } catch (e) { console.log(e); } }); socket.on(\u0026#39;data\u0026#39;, async data =\u0026gt; { if (client?._trainer) { iWrite(data); io.volatile.to(client._trainer.socketID).emit(\u0026#39;console\u0026#39;, data); } }); }); } The aforementioned behavior is demonstrated in the code snippet above. One nasty bug that bothered me for a couple of days was the client device sending data to an old socket id if the dashboard (frontend application) established a socket connection after the client or if the trainer refreshed the page. That happened because the socket id of the dashboard was now outdated since a new socket connection was established and a new id was generated. In addition to code being block scoped in a socket.io event that made it impossible to solve with the libraries built-in functions. Hitting the database at preset intervals was a big no. Imagine having tens of clients doing the same thing. That quickly brings the requests to thousands per minute.\nRedis pub-sub came to the rescue! By subscribing to the updateSocketID event whenever a client connects and triggering it when a dashboard (re)connects, excess database reads are avoided. The only thing the subscriber does on that event trigger is check if his trainer changed id and pick up the new one from the store if so.\nThe rest of the server is pretty basic. Field validation with celebrate, session and user management with bcrypt/passport/cookie-session, cors and rate limiter for security reasons, and the list goes on. Check the repo for more details.\nDatabases Ichnaea utilizes three databases, each for its distinct purpose.\nMongoDB MongoDB was chosen for the user store. Two models were created, one for the user logging to the dashboard and one to represent each distinct athlete. Each model has some basic identification features and the all-important socketID. This field stores the last socket connection id, useful to stream data only to the intended recipient.\nAnother important field is the _trainer, found only in the athletes model. With that we keep a reference to the trainer of the athlete, that is the user _id. To retrieve the trainer details at the same time as the athlete\u0026rsquo;s details, we can use the .populate() function like so.\nclient = await Athlete.findOne({id}).populate(\u0026#39;_trainer\u0026#39;); InfluxDB A time-series database was a precise fit for the serial data generated from the sensors. Writing data to influx is as easy as defining a new Point with the data you want to save. InfluxDB comes with a rich dashboard with various data display capabilities. A Grafana instance is attached to the Influx instance, so you can squeeze out every metric from Ichnaea.\nconst imuPoint = new Point(pointName) .tag(\u0026#39;client\u0026#39;, uuid) .tag(\u0026#39;sensor\u0026#39;, \u0026#39;IMU\u0026#39;) .floatField(\u0026#39;temperature\u0026#39;, temperature) .floatField(\u0026#39;pitch\u0026#39;, pitch) .floatField(\u0026#39;roll\u0026#39;, roll) .floatField(\u0026#39;yaw\u0026#39;, yaw) .floatField(\u0026#39;acceleration\u0026#39;, acceleration) .floatField(\u0026#39;inclination\u0026#39;, inclination) .floatField(\u0026#39;orientation\u0026#39;, orientation); writeApi.writePoint(imuPoint); Redis Redis is used as a publish-subscribe mechanism in two places. The first place we make use of pubsub is updating the socket id dynamically when the dashboard reconnects, as mentioned a couple of sections before. Secondly, it is used as a socket.io adapter, helping when scaling up with multiple instances of the backend. This way we can broadcast a message to multiple clients even if they are connected to a different server. You can read more about that here.\nFrontend Last but not least, the Frontend is built on Vue.js, and is a single page application that makes the dashboard of Ichnaea. Vue is a lovely framework that is extensible with things like routing and state management. I won\u0026rsquo;t go into any details about the framework - feel free to browse the docs.\nI tried to make the dashboard as realistic as possible. It has the following features:\nView the latest README from Github right in the app. Make changes to the user and athlete profile. Adopt or drop athletes. View, search, or inspect athletes and their details. View athlete data in a table or model in real-time. Not something overly impressive, but some basic functionality that would form the building blocks of a real-world app.\nThe main point of the dashboard is the model animation. I fixed a Mixamo model to a Three.js scene, and with the help of quaternions, the model animation is a breeze. I used the build in function from Three.js for the conversion from Euler, setFromEuler(). Taking a look inside setFromEuler(), we can see that the conversion is simple enough.\nconst cos = Math.cos; const sin = Math.sin; const c1 = cos(x / 2); const c2 = cos(y / 2); const c3 = cos(z / 2); const s1 = sin(x / 2); const s2 = sin(y / 2); const s3 = sin(z / 2); this._x = s1 * c2 * c3 + c1 * s2 * s3; this._y = c1 * s2 * c3 - s1 * c2 * s3; this._z = c1 * c2 * s3 + s1 * s2 * c3; this._w = c1 * c2 * c3 - s1 * s2 * s3; I also keep a reference of body parts, and set its position as so.\nthis.head = object.getObjectByName(\u0026#39;mixamorig1Head\u0026#39;); this.head.quaternion.setFromEuler(new Euler(this.roll, this.yaw, this.pitch)); Demo Placing the micro-controller on top of the head and making three discreet movements from neutral to left, back, and upwards right, maps with precision the physical movement and accurately depicts it on the model while testing all three axes, roll-pitch-yaw.\nLet\u0026rsquo;s try the same but for another body part, this time the left arm. Starting from a relaxed hanging position we pull the arm upwards making a slow circular motion and then returning to the starting point. This time due to us deliberately making a slow movement, we can now see the drift introduced from the gyroscope in the yaw angle. That skews with all the readings and thus placing the hand in a wrong final position.\nConclusion This was an overview of Ichnaea, what I thought were the most interesting points, and by no means the full picture. What needs to be improved? Abstracting the sensor positioning logic to a UI where the user can drag and drop each sensor to the respective body and not hard-coding it. Making the client device smaller and self-sufficient with a battery and extensive connectivity options. Lastly, some ML with smart alerts and corrections would provide smart insights.\n","permalink":"https://haris.razis.com/posts/ichnaea/","summary":"Scalable IoT solution for real-time body position tracking","title":"Body position tracking"}]